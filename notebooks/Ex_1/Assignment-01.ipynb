{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9deecd14-d6ed-4c7d-9fc6-b3f1cc3f3d89",
   "metadata": {},
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    "table {display: block;}\n",
    "td {\n",
    "  font-size: 20px\n",
    "}\n",
    ".rendered_html { font-size: 20px; }\n",
    "*{ line-height: 200%; }\n",
    "\n",
    ".task {\n",
    "    color: green;\n",
    "    font-weight: bold;\n",
    "    background-color: #f0f0f0;\n",
    "    padding: 2px;\n",
    "}\n",
    "\n",
    ".question {\n",
    "    color: red;\n",
    "    font-size: 20px;\n",
    "    font-weight: bold;\n",
    "}\n",
    "    span.task {\n",
    "    color: green !important;\n",
    "    font-weight: bold;\n",
    "    background-color: #f0f0f0;\n",
    "    padding: 2px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f572034b-2482-44ee-8a67-e45f0743fa9b",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Natural Language Processing and the Web WS25/26</span>  \n",
    "# Assignment 01\n",
    "### Deadline: Friday October 24\n",
    "## <span class='question'>Submission Instructions</span>\n",
    "- Make only one submission per group\n",
    "- Provide Full Names of all group members as displayed in Moodle\n",
    "- Submit the notebook, including both the solution and its output. \n",
    "- Do not submit any output files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3825acb-93d0-43b7-8846-c98093565266",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Group Members:</span>\n",
    "\n",
    "Gaurika Chopra\n",
    "\n",
    "Nasrul Huda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a8b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Importing Libraries\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Download required NLTK data files (only once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306f838-ec47-4455-b066-cd1148dbef78",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-01</span> - 10%\n",
    "Update the `scrape_hacker_news` function from Example 02, Practice Class 01. The current function only scrapes article titles from the first page of Hacker News\n",
    "\n",
    "1. Modify the function to scrape articles and their URLs from multiple pages, add a parameter to allow the user to specify the maximum number of pages to scrape, set its default value to 1\n",
    "2. Add  a parameter to allow the user to specify the file name to save the extracted results in a csv format, make sure to strip any trailing and leading whitespaces characters before saving the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d499a7a-eba3-40b3-90e1-a3681b8c4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping page 1: https://news.ycombinator.com/ ---\n",
      "\n",
      "--- Scraping page 2: https://news.ycombinator.com/?p=2 ---\n",
      "\n",
      "--- Scraping page 3: https://news.ycombinator.com/?p=3 ---\n",
      "\n",
      "--- Scraping page 4: https://news.ycombinator.com/?p=4 ---\n",
      "\n",
      "--- Scraping page 5: https://news.ycombinator.com/?p=5 ---\n",
      "\n",
      "--- Scraping page 6: https://news.ycombinator.com/?p=6 ---\n",
      "\n",
      "--- Scraping page 7: https://news.ycombinator.com/?p=7 ---\n",
      "\n",
      "--- Scraping complete! Extracted 210 articles ---\n",
      "Results saved to: hn_articles.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# Base URL\n",
    "BASE_URL = \"https://news.ycombinator.com/\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "\n",
    "def scrape_hacker_news(start_url=BASE_URL, max_pages=1, output_file=\"hacker_news.csv\"):\n",
    "    \"\"\"\n",
    "    Scrapes article titles and URLs from Hacker News across multiple pages\n",
    "    and saves the results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        start_url (str): Starting URL of Hacker News.\n",
    "        max_pages (int): Maximum number of pages to scrape. Default is 1.\n",
    "        output_file (str): File name to save the extracted results in CSV format.\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "    current_url = start_url\n",
    "\n",
    "    try:\n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"\\n--- Scraping page {page}: {current_url} ---\")\n",
    "\n",
    "            # Fetch the page\n",
    "            response = requests.get(current_url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            # Extract all article rows\n",
    "            articles = soup.find_all(class_=\"athing\")\n",
    "            if not articles:\n",
    "                print(\"No articles found on this page. Stopping.\")\n",
    "                break\n",
    "\n",
    "            # Process articles\n",
    "            for article in articles:\n",
    "                title_tag = article.find(class_=\"titleline\").find(\"a\")\n",
    "                title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "                url = title_tag[\"href\"].strip() if title_tag else \"N/A\"\n",
    "                all_articles.append({\"title\": title, \"url\": url})\n",
    "\n",
    "            # Find the \"More\" link to get next page\n",
    "            more_link = soup.find(\"a\", string=\"More\")\n",
    "            if more_link and more_link.get(\"href\"):\n",
    "                current_url = BASE_URL + more_link[\"href\"]\n",
    "            else:\n",
    "                print(\"No further pages found.\")\n",
    "                break\n",
    "\n",
    "        # Save to CSV\n",
    "        output_file = output_file.strip()  # Remove any leading/trailing spaces\n",
    "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"url\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_articles)\n",
    "\n",
    "        print(f\"\\n--- Scraping complete! Extracted {len(all_articles)} articles ---\")\n",
    "        print(f\"Results saved to: {output_file}\\n\")\n",
    "\n",
    "        return all_articles\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching {current_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hacker_news(BASE_URL, max_pages=7, output_file=\"hn_articles.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f5db4-dc6c-4396-b3d2-d0135f708365",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-02</span> - 50%\n",
    "Use `scrape_hacker_news` to scrape 15 pages for this task and save them to `hackernews-articles.csv`, assume the file is structured as follows:\n",
    "```\n",
    "Title, URL\n",
    "title1, url1\n",
    "title2, url2\n",
    "...\n",
    "```\n",
    "\n",
    "Perform the following tasks on the article titles:\n",
    "\n",
    "#### 1. Build a lookup index using words as keys\n",
    "- Tokenize the original titles into individual words/tokens.\n",
    "- Collect a list of all unique words that appear across all articles\n",
    "- Create a dictionary where:\n",
    "     * Key = token/word\n",
    "     * Value = list of article IDs (or indices) containing that word\n",
    "     \n",
    "       For Example\n",
    "         ```\n",
    "         {\n",
    "            \"AI\": [0, 2, 5], # ids/indices of articles containing 'AI'\n",
    "            \"Python\": [0, 2, 9], # ids/indices of articles containing 'Python'\n",
    "         }\n",
    "        ```\n",
    "       > Here [0, 2, 5, 9] represents index of articles from `hackernews-articles.csv`. \n",
    "\n",
    "-  Save the resulting dictionary as `word-index.json` \n",
    "\n",
    "#### 2. Repeat the same steps to build two additional lookup indices\n",
    "\n",
    "- Stem Index: Apply a stemmer (e.g., PorterStemmer) to each token. Use the stemmed form of the token as the dictionary key. Save the resulting dictionary as `stem-index.json`.\n",
    "- Lemma Index: Apply a lemmatizer (e.g., WordNetLemmatizer) using appropriate POS tags. Use the lemmatized form of each token as the dictionary key. Save the resulting dictionary as `lemma-index.json`.\n",
    "\n",
    "#### 3. Print vocab size of all three indices\n",
    "After building all three indices (word-index, stem-index, lemma-index), print the vocabulary size (number of unique keys) for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b392f6-94c9-45f2-a7d6-8a3b5a6c198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping page 1: https://news.ycombinator.com/ ---\n",
      "\n",
      "--- Scraping page 2: https://news.ycombinator.com/?p=2 ---\n",
      "\n",
      "--- Scraping page 3: https://news.ycombinator.com/?p=3 ---\n",
      "\n",
      "--- Scraping page 4: https://news.ycombinator.com/?p=4 ---\n",
      "\n",
      "--- Scraping page 5: https://news.ycombinator.com/?p=5 ---\n",
      "\n",
      "--- Scraping page 6: https://news.ycombinator.com/?p=6 ---\n",
      "\n",
      "--- Scraping page 7: https://news.ycombinator.com/?p=7 ---\n",
      "\n",
      "--- Scraping page 8: https://news.ycombinator.com/?p=8 ---\n",
      "\n",
      "--- Scraping page 9: https://news.ycombinator.com/?p=9 ---\n",
      "\n",
      "--- Scraping page 10: https://news.ycombinator.com/?p=10 ---\n",
      "\n",
      "--- Scraping page 11: https://news.ycombinator.com/?p=11 ---\n",
      "\n",
      "--- Scraping page 12: https://news.ycombinator.com/?p=12 ---\n",
      "\n",
      "--- Scraping page 13: https://news.ycombinator.com/?p=13 ---\n",
      "\n",
      "--- Scraping page 14: https://news.ycombinator.com/?p=14 ---\n",
      "\n",
      "--- Scraping page 15: https://news.ycombinator.com/?p=15 ---\n",
      "\n",
      "--- Scraping complete! Extracted 450 articles ---\n",
      "Results saved to: hackernews-articles.csv\n",
      "\n",
      "Loaded 450 article titles.\n",
      "\n",
      "--- Vocabulary Sizes ---\n",
      "Word Index  : 1966 unique words\n",
      "Stem Index  : 1765 unique stems\n",
      "Lemma Index : 1812 unique lemmas\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 1: Scrape 15 pages and save to hackernews-articles.csv\n",
    "# ------------------------------------------------------\n",
    "\n",
    "scrape_hacker_news(max_pages=15, output_file=\"hackernews-articles.csv\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 2: Read the CSV data\n",
    "# ------------------------------------------------------\n",
    "\n",
    "titles = []\n",
    "with open(\"hackernews-articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        titles.append(row[\"title\"])\n",
    "\n",
    "print(f\"Loaded {len(titles)} article titles.\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 3: Helper functions\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase, remove punctuation, and tokenize.\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(re.sub(r\"[^a-zA-Z0-9]+\", \" \", text))\n",
    "    return [t for t in tokens if t.isalnum()]\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS for better lemmatization.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 4: Build indices\n",
    "# ------------------------------------------------------\n",
    "\n",
    "word_index = {}\n",
    "stem_index = {}\n",
    "lemma_index = {}\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for idx, title in enumerate(titles):\n",
    "    tokens = preprocess_text(title)\n",
    "\n",
    "    # --- Word Index ---\n",
    "    for token in tokens:\n",
    "        word_index.setdefault(token, []).append(idx)\n",
    "\n",
    "    # --- Stem Index ---\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        stem_index.setdefault(stem, []).append(idx)\n",
    "\n",
    "    # --- Lemma Index ---\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    for token, tag in pos_tags:\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "        lemma_index.setdefault(lemma, []).append(idx)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 5: Save indices as JSON files\n",
    "# ------------------------------------------------------\n",
    "\n",
    "with open(\"word-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"stem-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stem_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"lemma-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lemma_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# STEP 6: Print vocabulary sizes\n",
    "# ------------------------------------------------------\n",
    "\n",
    "print(\"\\n--- Vocabulary Sizes ---\")\n",
    "print(f\"Word Index  : {len(word_index)} unique words\")\n",
    "print(f\"Stem Index  : {len(stem_index)} unique stems\")\n",
    "print(f\"Lemma Index : {len(lemma_index)} unique lemmas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73eb35-45db-4d91-bb51-17a05666fc8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span class='task'>TASK-03</span> - 25%\n",
    "Assuming you have successfully built the lookup indices:\n",
    "\n",
    "#### Define a search function\n",
    "The function should:\n",
    "- Take a keyword as an argument.\n",
    "- Search for matching articles in all three indices — original, stemmed, and lemmatized.\n",
    "- For each index: \n",
    "  - Return the titles of those matching articles from `hackernews-articles.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cb102dd-88d9-4bf3-b6cc-436f814e6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper function for WordNet POS tagging\n",
    "# ------------------------------------------------------------\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS for better lemmatization.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load Data\n",
    "# ------------------------------------------------------------\n",
    "# Load the article titles from CSV\n",
    "titles = []\n",
    "with open(\"hackernews-articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        titles.append(row[\"title\"])\n",
    "\n",
    "# Load indices\n",
    "with open(\"word-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "with open(\"stem-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stem_index = json.load(f)\n",
    "\n",
    "with open(\"lemma-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lemma_index = json.load(f)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define the Search Function\n",
    "# ------------------------------------------------------------\n",
    "def search_keyword(keyword):\n",
    "    \"\"\"\n",
    "    Search for a keyword in all three indices and return matching article titles.\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower().strip()\n",
    "\n",
    "    # Get stem and lemma forms\n",
    "    stem = stemmer.stem(keyword)\n",
    "    pos_tag = nltk.pos_tag([keyword])[0][1]\n",
    "    lemma = lemmatizer.lemmatize(keyword, get_wordnet_pos(pos_tag))\n",
    "\n",
    "    print(f\"\\n Searching for: '{keyword}'\")\n",
    "    print(f\"Stemmed form : {stem}\")\n",
    "    print(f\"Lemmatized form : {lemma}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Search in Word Index\n",
    "    if keyword in word_index:\n",
    "        indices = word_index[keyword]\n",
    "        results[\"word_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"word_index\"] = []\n",
    "\n",
    "    # Search in Stem Index\n",
    "    if stem in stem_index:\n",
    "        indices = stem_index[stem]\n",
    "        results[\"stem_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"stem_index\"] = []\n",
    "\n",
    "    # Search in Lemma Index\n",
    "    if lemma in lemma_index:\n",
    "        indices = lemma_index[lemma]\n",
    "        results[\"lemma_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"lemma_index\"] = []\n",
    "\n",
    "    # Print results neatly\n",
    "    for idx_name, titles_list in results.items():\n",
    "        print(f\"\\n--- Matches in {idx_name} ({len(titles_list)} results) ---\")\n",
    "        for t in titles_list[:10]:  # show first 10\n",
    "            print(f\"• {t}\")\n",
    "        if len(titles_list) > 10:\n",
    "            print(f\"... and {len(titles_list) - 10} more\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab4bc1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Searching for: 'cloud'\n",
      "Stemmed form : cloud\n",
      "Lemmatized form : cloud\n",
      "\n",
      "--- Matches in word_index (2 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n",
      "• Hetzner: The Simple Cloud just got more flexible and more affordable\n",
      "\n",
      "--- Matches in stem_index (2 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n",
      "• Hetzner: The Simple Cloud just got more flexible and more affordable\n",
      "\n",
      "--- Matches in lemma_index (2 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n",
      "• Hetzner: The Simple Cloud just got more flexible and more affordable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system',\n",
       "  'Hetzner: The Simple Cloud just got more flexible and more affordable'],\n",
       " 'stem_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system',\n",
       "  'Hetzner: The Simple Cloud just got more flexible and more affordable'],\n",
       " 'lemma_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system',\n",
       "  'Hetzner: The Simple Cloud just got more flexible and more affordable']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_keyword(\"cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784d5a8-5b63-41dd-bdb2-d79638b9055c",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-04</span> - 15%\n",
    "Randomly select 5 keywords from your list of article titles and use your search function to compare results across all three indices (original, stemmed, and lemmatized):\n",
    "1. Print the number of matches and the titles of the articles for each keyword. You may display them as a table or simply print them as text.\n",
    "\n",
    "2. Did stemming or lemmatization help retrieve more relevant articles?\n",
    "3. Were there any false matches (irrelevant results)?\n",
    "4. When might stemming be better than lemmatization (or vice versa)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "426d4cdb-86c8-4f1b-8831-8dbe67a590d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Randomly selected keywords: ['level', 'passes', 'threaded', 'multithreading', 'confirms']\n",
      "\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'level'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'level'\n",
      "Stemmed form : level\n",
      "Lemmatized form : level\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• LLMs are getting better at character-level text manipulation\n",
      "\n",
      "--- Matches in stem_index (2 results) ---\n",
      "• LLMs are getting better at character-level text manipulation\n",
      "• Medical student ate 700 eggs in a month and his cholesterol levels dropped\n",
      "\n",
      "--- Matches in lemma_index (2 results) ---\n",
      "• LLMs are getting better at character-level text manipulation\n",
      "• Medical student ate 700 eggs in a month and his cholesterol levels dropped\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'passes'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'passes'\n",
      "Stemmed form : pass\n",
      "Lemmatized form : pass\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Grandmaster, Popular Commentator Daniel Naroditsky Tragically Passes Away at 29\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• Grandmaster, Popular Commentator Daniel Naroditsky Tragically Passes Away at 29\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• Grandmaster, Popular Commentator Daniel Naroditsky Tragically Passes Away at 29\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'threaded'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'threaded'\n",
      "Stemmed form : thread\n",
      "Lemmatized form : thread\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Old Is Gold: Optimizing Single-Threaded Applications with Exgen-Malloc\n",
      "\n",
      "--- Matches in stem_index (3 results) ---\n",
      "• How to stop Linux threads cleanly\n",
      "• The Death of Thread per Core\n",
      "• Old Is Gold: Optimizing Single-Threaded Applications with Exgen-Malloc\n",
      "\n",
      "--- Matches in lemma_index (3 results) ---\n",
      "• How to stop Linux threads cleanly\n",
      "• The Death of Thread per Core\n",
      "• Old Is Gold: Optimizing Single-Threaded Applications with Exgen-Malloc\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'multithreading'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'multithreading'\n",
      "Stemmed form : multithread\n",
      "Lemmatized form : multithreading\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Deterministic multithreading is hard (2024)\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• Deterministic multithreading is hard (2024)\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• Deterministic multithreading is hard (2024)\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'confirms'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'confirms'\n",
      "Stemmed form : confirm\n",
      "Lemmatized form : confirms\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Trump DOE confirms it's canceling over $700M in manufacturing grants\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• Trump DOE confirms it's canceling over $700M in manufacturing grants\n",
      "\n",
      "--- Matches in lemma_index (0 results) ---\n",
      "\n",
      "================================================================================\n",
      " Summary of Match Counts\n",
      "================================================================================\n",
      "Keyword        Word Index     Stem Index     Lemma Index    \n",
      "--------------------------------------------------------------------------------\n",
      "level          1              2              2              \n",
      "passes         1              1              1              \n",
      "threaded       1              3              3              \n",
      "multithreading 1              1              1              \n",
      "confirms       1              1              0              \n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 1: Extract keywords (unique words) from article titles\n",
    "# ------------------------------------------------------------\n",
    "keywords = set()\n",
    "\n",
    "for title in titles:\n",
    "    words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", title.lower())  # 3+ letter words\n",
    "    keywords.update(words)\n",
    "\n",
    "# Randomly select 5 unique keywords\n",
    "sample_keywords = random.sample(list(keywords), 5)\n",
    "print(f\"\\n Randomly selected keywords: {sample_keywords}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 2: Search each keyword and print comparison\n",
    "# ------------------------------------------------------------\n",
    "comparison_results = {}\n",
    "\n",
    "for word in sample_keywords:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n Keyword: '{word}'\")\n",
    "    print(\"=\" * 80)\n",
    "    results = search_keyword(word)\n",
    "\n",
    "    # Store counts for summary\n",
    "    comparison_results[word] = {\n",
    "        \"Word Index\": len(results[\"word_index\"]),\n",
    "        \"Stem Index\": len(results[\"stem_index\"]),\n",
    "        \"Lemma Index\": len(results[\"lemma_index\"]),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 3: Summary Table\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Summary of Match Counts\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Keyword':<15}{'Word Index':<15}{'Stem Index':<15}{'Lemma Index':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for kw, counts in comparison_results.items():\n",
    "    print(f\"{kw:<15}{counts['Word Index']:<15}{counts['Stem Index']:<15}{counts['Lemma Index']:<15}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
