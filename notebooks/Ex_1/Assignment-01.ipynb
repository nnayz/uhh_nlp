{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f572034b-2482-44ee-8a67-e45f0743fa9b",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Natural Language Processing and the Web WS25/26</span>  \n",
    "# Assignment 01\n",
    "### Deadline: Friday October 24\n",
    "## <span class='question'>Submission Instructions</span>\n",
    "- Make only one submission per group\n",
    "- Provide Full Names of all group members as displayed in Moodle\n",
    "- Submit the notebook, including both the solution and its output. \n",
    "- Do not submit any output files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3825acb-93d0-43b7-8846-c98093565266",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Group Members:</span>\n",
    "\n",
    "Gaurika Chopra\n",
    "\n",
    "Nasrul Huda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a8b9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eaec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings constants\n",
    "BASE_URL = \"https://news.ycombinator.com/\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306f838-ec47-4455-b066-cd1148dbef78",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-01</span> - 10%\n",
    "Update the `scrape_hacker_news` function from Example 02, Practice Class 01. The current function only scrapes article titles from the first page of Hacker News\n",
    "\n",
    "1. Modify the function to scrape articles and their URLs from multiple pages, add a parameter to allow the user to specify the maximum number of pages to scrape, set its default value to 1\n",
    "2. Add  a parameter to allow the user to specify the file name to save the extracted results in a csv format, make sure to strip any trailing and leading whitespaces characters before saving the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d499a7a-eba3-40b3-90e1-a3681b8c4320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping page 1: https://news.ycombinator.com/ ---\n",
      "\n",
      "--- Scraping complete! Extracted 30 articles ---\n",
      "Results saved to: None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'VST3 audio plugin format is now MIT',\n",
       "  'url': 'https://forums.steinberg.net/t/vst-3-8-0-sdk-released/1011988'},\n",
       " {'title': 'PyTorch Monarch',\n",
       "  'url': 'https://pytorch.org/blog/introducing-pytorch-monarch/'},\n",
       " {'title': 'Google flags Immich sites as dangerous',\n",
       "  'url': 'https://immich.app/blog/google-flags-immich-as-dangerous'},\n",
       " {'title': 'C64 Blood Money', 'url': 'https://lemmings.info/c64-blood-money/'},\n",
       " {'title': 'Programming with Less Than Nothing',\n",
       "  'url': 'https://joshmoody.org/blog/programming-with-less-than-nothing/'},\n",
       " {'title': 'Nango (YC W23) is hiring Staff Back end Engs (remote)',\n",
       "  'url': 'https://www.nango.dev/careers'},\n",
       " {'title': 'The Game Theory of How Algorithms Can Drive Up Prices',\n",
       "  'url': 'https://www.quantamagazine.org/the-game-theory-of-how-algorithms-can-drive-up-prices-20251022/'},\n",
       " {'title': 'Radios, how do they work? (2024)',\n",
       "  'url': 'https://lcamtuf.substack.com/p/radios-how-do-they-work'},\n",
       " {'title': 'Egg prices vs. Consumer Price Index since 1980',\n",
       "  'url': 'https://fred.stlouisfed.org/graph/?g=1Nm5b'},\n",
       " {'title': 'SpaceX disables 2,500 Starlink terminals allegedly used by Asian scam centers',\n",
       "  'url': 'https://arstechnica.com/tech-policy/2025/10/starlink-blocks-2500-dishes-allegedly-used-by-myanmars-notorious-scam-centers/'},\n",
       " {'title': 'Scripts I wrote that I use all the time',\n",
       "  'url': 'https://evanhahn.com/scripts-i-wrote-that-i-use-all-the-time/'},\n",
       " {'title': 'Run interactive commands in Gemini CLI',\n",
       "  'url': 'https://developers.googleblog.com/en/say-hello-to-a-new-level-of-interactivity-in-gemini-cli/'},\n",
       " {'title': 'Willow quantum chip demonstrates verifiable quantum advantage on hardware',\n",
       "  'url': 'https://blog.google/technology/research/quantum-echoes-willow-verifiable-quantum-advantage/'},\n",
       " {'title': \"Accessing Max Verstappen's passport and PII through FIA bugs\",\n",
       "  'url': 'https://ian.sh/fia'},\n",
       " {'title': 'Power-over-Skin: Full-Body Wearables Powered by Intra-Body RF Energy (2024)',\n",
       "  'url': 'https://dl.acm.org/doi/10.1145/3654777.3676394'},\n",
       " {'title': 'JMAP for Calendars, Contacts and Files Now in Stalwart',\n",
       "  'url': 'https://stalw.art/blog/jmap-collaboration/'},\n",
       " {'title': 'Karpathy on DeepSeek-OCR paper: Are pixels better inputs to LLMs than text?',\n",
       "  'url': 'https://twitter.com/karpathy/status/1980397031542989305'},\n",
       " {'title': 'Ovi: Twin backbone cross-modal fusion for audio-video generation',\n",
       "  'url': 'https://github.com/character-ai/Ovi'},\n",
       " {'title': 'When You Get to Be Smart Writing a Macro',\n",
       "  'url': 'https://tonsky.me/blog/hashp/'},\n",
       " {'title': 'Show HN: Silly Morse code chat app using WebSockets',\n",
       "  'url': 'https://noamtamir.github.io/morwse/'},\n",
       " {'title': 'Why SSA Compilers?',\n",
       "  'url': 'https://mcyoung.xyz/2025/10/21/ssa-1/'},\n",
       " {'title': 'Element: setHTML() method',\n",
       "  'url': 'https://developer.mozilla.org/en-US/docs/Web/API/Element/setHTML'},\n",
       " {'title': 'Play abstract strategy board games online with friends or against bots',\n",
       "  'url': 'https://abstractboardgames.com/'},\n",
       " {'title': 'A Distributed Emulation Environment for In-Memory Computing Systems',\n",
       "  'url': 'https://www.arxiv.org/pdf/2510.08257'},\n",
       " {'title': 'The first interstellar software update: The hack that saved Voyager 1 [video]',\n",
       "  'url': 'https://www.youtube.com/watch?v=p0K7u3B_8rY'},\n",
       " {'title': \"Rivian's TM-B electric bike\",\n",
       "  'url': 'https://www.theverge.com/news/804157/rivian-tm-b-electric-bike-price-specs-helmet-quad'},\n",
       " {'title': 'Glasses-free 3D using webcam head tracking',\n",
       "  'url': 'https://assetstore.unity.com/packages/tools/camera/vr-without-glasses-for-webgl-332314'},\n",
       " {'title': \"Derek Sivers's database and web apps\",\n",
       "  'url': 'https://github.com/sivers/sivers'},\n",
       " {'title': 'Common yeast can survive Martian conditions',\n",
       "  'url': 'https://phys.org/news/2025-10-common-yeast-survive-martian-conditions.html'},\n",
       " {'title': 'The mild mannered Englishman who was the most prolific ghost hunter',\n",
       "  'url': 'https://lithub.com/the-mild-mannered-englishman-who-was-the-worlds-most-prolific-ghost-hunter/'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_hacker_news(start_url=BASE_URL, max_pages=1, output_file=None):\n",
    "    all_articles = []\n",
    "    current_url = start_url\n",
    "\n",
    "    try:\n",
    "        for page in range(1, max_pages + 1):\n",
    "            print(f\"\\n--- Scraping page {page}: {current_url} ---\")\n",
    "\n",
    "            response = requests.get(current_url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            articles = soup.find_all(class_=\"athing\")\n",
    "            if not articles:\n",
    "                print(\"No articles found on this page. Stopping.\")\n",
    "                break\n",
    "\n",
    "            for article in articles:\n",
    "                title_tag = article.find(class_=\"titleline\").find(\"a\")\n",
    "                title = title_tag.text.strip() if title_tag else \"N/A\"\n",
    "                url = title_tag[\"href\"].strip() if title_tag else \"N/A\"\n",
    "                all_articles.append({\"title\": title, \"url\": url})\n",
    "\n",
    "            # Find the \"More\" link to get next page\n",
    "            more_link = soup.find(\"a\", string=\"More\")\n",
    "            if more_link and more_link.get(\"href\"):\n",
    "                current_url = BASE_URL + more_link[\"href\"]\n",
    "            else:\n",
    "                print(\"No further pages found.\")\n",
    "                break\n",
    "\n",
    "        # Save to CSV\n",
    "        if output_file:\n",
    "            output_file = output_file.strip()\n",
    "            with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=[\"title\", \"url\"])\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_articles)\n",
    "\n",
    "        print(f\"\\n--- Scraping complete! Extracted {len(all_articles)} articles ---\")\n",
    "        print(f\"Results saved to: {output_file}\\n\")\n",
    "\n",
    "        return all_articles\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while fetching {current_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "scrape_hacker_news(start_url=BASE_URL, max_pages=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f5db4-dc6c-4396-b3d2-d0135f708365",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-02</span> - 50%\n",
    "Use `scrape_hacker_news` to scrape 15 pages for this task and save them to `hackernews-articles.csv`, assume the file is structured as follows:\n",
    "```\n",
    "Title, URL\n",
    "title1, url1\n",
    "title2, url2\n",
    "...\n",
    "```\n",
    "\n",
    "Perform the following tasks on the article titles:\n",
    "\n",
    "#### 1. Build a lookup index using words as keys\n",
    "- Tokenize the original titles into individual words/tokens.\n",
    "- Collect a list of all unique words that appear across all articles\n",
    "- Create a dictionary where:\n",
    "     * Key = token/word\n",
    "     * Value = list of article IDs (or indices) containing that word\n",
    "     \n",
    "       For Example\n",
    "         ```\n",
    "         {\n",
    "            \"AI\": [0, 2, 5], # ids/indices of articles containing 'AI'\n",
    "            \"Python\": [0, 2, 9], # ids/indices of articles containing 'Python'\n",
    "         }\n",
    "        ```\n",
    "       > Here [0, 2, 5, 9] represents index of articles from `hackernews-articles.csv`. \n",
    "\n",
    "-  Save the resulting dictionary as `word-index.json` \n",
    "\n",
    "#### 2. Repeat the same steps to build two additional lookup indices\n",
    "\n",
    "- Stem Index: Apply a stemmer (e.g., PorterStemmer) to each token. Use the stemmed form of the token as the dictionary key. Save the resulting dictionary as `stem-index.json`.\n",
    "- Lemma Index: Apply a lemmatizer (e.g., WordNetLemmatizer) using appropriate POS tags. Use the lemmatized form of each token as the dictionary key. Save the resulting dictionary as `lemma-index.json`.\n",
    "\n",
    "#### 3. Print vocab size of all three indices\n",
    "After building all three indices (word-index, stem-index, lemma-index), print the vocabulary size (number of unique keys) for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b392f6-94c9-45f2-a7d6-8a3b5a6c198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping page 1: https://news.ycombinator.com/ ---\n",
      "\n",
      "--- Scraping page 2: https://news.ycombinator.com/?p=2 ---\n",
      "\n",
      "--- Scraping page 3: https://news.ycombinator.com/?p=3 ---\n",
      "\n",
      "--- Scraping page 4: https://news.ycombinator.com/?p=4 ---\n",
      "\n",
      "--- Scraping page 5: https://news.ycombinator.com/?p=5 ---\n",
      "\n",
      "--- Scraping page 6: https://news.ycombinator.com/?p=6 ---\n",
      "\n",
      "--- Scraping page 7: https://news.ycombinator.com/?p=7 ---\n",
      "\n",
      "--- Scraping page 8: https://news.ycombinator.com/?p=8 ---\n",
      "\n",
      "--- Scraping page 9: https://news.ycombinator.com/?p=9 ---\n",
      "\n",
      "--- Scraping page 10: https://news.ycombinator.com/?p=10 ---\n",
      "\n",
      "--- Scraping page 11: https://news.ycombinator.com/?p=11 ---\n",
      "\n",
      "--- Scraping page 12: https://news.ycombinator.com/?p=12 ---\n",
      "\n",
      "--- Scraping page 13: https://news.ycombinator.com/?p=13 ---\n",
      "\n",
      "--- Scraping page 14: https://news.ycombinator.com/?p=14 ---\n",
      "\n",
      "--- Scraping page 15: https://news.ycombinator.com/?p=15 ---\n",
      "\n",
      "--- Scraping complete! Extracted 450 articles ---\n",
      "Results saved to: hackernews-articles.csv\n",
      "\n",
      "Loaded 450 article titles.\n",
      "\n",
      "--- Vocabulary Sizes ---\n",
      "Word Index  : 1937 unique words\n",
      "Stem Index  : 1744 unique stems\n",
      "Lemma Index : 1780 unique lemmas\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "scrape_hacker_news(max_pages=15, output_file=\"hackernews-articles.csv\")\n",
    "\n",
    "# Read the CSV data\n",
    "titles = []\n",
    "with open(\"hackernews-articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        titles.append(row[\"title\"])\n",
    "\n",
    "print(f\"Loaded {len(titles)} article titles.\")\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(re.sub(r\"[^a-zA-Z0-9]+\", \" \", text))\n",
    "    return [t for t in tokens if t.isalnum()]\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Build indices\n",
    "word_index = {}\n",
    "stem_index = {}\n",
    "lemma_index = {}\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for idx, title in enumerate(titles):\n",
    "    tokens = preprocess_text(title)\n",
    "\n",
    "    for token in tokens:\n",
    "        word_index.setdefault(token, []).append(idx)\n",
    "\n",
    "    for token in tokens:\n",
    "        stem = stemmer.stem(token)\n",
    "        stem_index.setdefault(stem, []).append(idx)\n",
    "\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    for token, tag in pos_tags:\n",
    "        lemma = lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "        lemma_index.setdefault(lemma, []).append(idx)\n",
    "\n",
    "# Save indices as JSON files\n",
    "\n",
    "with open(\"word-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"stem-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(stem_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"lemma-index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(lemma_index, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print vocabulary sizes\n",
    "print(\"\\n--- Vocabulary Sizes ---\")\n",
    "print(f\"Word Index  : {len(word_index)} unique words\")\n",
    "print(f\"Stem Index  : {len(stem_index)} unique stems\")\n",
    "print(f\"Lemma Index : {len(lemma_index)} unique lemmas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73eb35-45db-4d91-bb51-17a05666fc8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span class='task'>TASK-03</span> - 25%\n",
    "Assuming you have successfully built the lookup indices:\n",
    "\n",
    "#### Define a search function\n",
    "The function should:\n",
    "- Take a keyword as an argument.\n",
    "- Search for matching articles in all three indices — original, stemmed, and lemmatized.\n",
    "- For each index: \n",
    "  - Return the titles of those matching articles from `hackernews-articles.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb102dd-88d9-4bf3-b6cc-436f814e6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for WordNet POS tagging\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Map POS tag to WordNet POS for better lemmatization.\"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "# Load Data\n",
    "titles = []\n",
    "with open(\"hackernews-articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        titles.append(row[\"title\"])\n",
    "\n",
    "# Load indices\n",
    "with open(\"word-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_index = json.load(f)\n",
    "\n",
    "with open(\"stem-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stem_index = json.load(f)\n",
    "\n",
    "with open(\"lemma-index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lemma_index = json.load(f)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Define the Search Function\n",
    "def search_keyword(keyword):\n",
    "    \"\"\"\n",
    "    Search for a keyword in all three indices and return matching article titles.\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower().strip()\n",
    "\n",
    "    # Get stem and lemma forms\n",
    "    stem = stemmer.stem(keyword)\n",
    "    pos_tag = nltk.pos_tag([keyword])[0][1]\n",
    "    lemma = lemmatizer.lemmatize(keyword, get_wordnet_pos(pos_tag))\n",
    "\n",
    "    print(f\"\\n Searching for: '{keyword}'\")\n",
    "    print(f\"Stemmed form : {stem}\")\n",
    "    print(f\"Lemmatized form : {lemma}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Search in Word Index\n",
    "    if keyword in word_index:\n",
    "        indices = word_index[keyword]\n",
    "        results[\"word_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"word_index\"] = []\n",
    "\n",
    "    # Search in Stem Index\n",
    "    if stem in stem_index:\n",
    "        indices = stem_index[stem]\n",
    "        results[\"stem_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"stem_index\"] = []\n",
    "\n",
    "    # Search in Lemma Index\n",
    "    if lemma in lemma_index:\n",
    "        indices = lemma_index[lemma]\n",
    "        results[\"lemma_index\"] = [titles[i] for i in indices]\n",
    "    else:\n",
    "        results[\"lemma_index\"] = []\n",
    "\n",
    "    # Print results neatly\n",
    "    for idx_name, titles_list in results.items():\n",
    "        print(f\"\\n--- Matches in {idx_name} ({len(titles_list)} results) ---\")\n",
    "        for t in titles_list[:10]:  # show first 10\n",
    "            print(f\"• {t}\")\n",
    "        if len(titles_list) > 10:\n",
    "            print(f\"... and {len(titles_list) - 10} more\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4bc1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Searching for: 'cloud'\n",
      "Stemmed form : cloud\n",
      "Lemmatized form : cloud\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system'],\n",
       " 'stem_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system'],\n",
       " 'lemma_index': ['Alibaba Cloud says it cut Nvidia AI GPU use by 82% with new pooling system']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_keyword(\"cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784d5a8-5b63-41dd-bdb2-d79638b9055c",
   "metadata": {},
   "source": [
    "## <span class='task'>TASK-04</span> - 15%\n",
    "Randomly select 5 keywords from your list of article titles and use your search function to compare results across all three indices (original, stemmed, and lemmatized):\n",
    "1. Print the number of matches and the titles of the articles for each keyword. You may display them as a table or simply print them as text.\n",
    "\n",
    "2. Did stemming or lemmatization help retrieve more relevant articles?\n",
    "3. Were there any false matches (irrelevant results)?\n",
    "4. When might stemming be better than lemmatization (or vice versa)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "426d4cdb-86c8-4f1b-8831-8dbe67a590d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Randomly selected keywords: ['maduro', 'models', 'intel', 'diffs', 'pure']\n",
      "\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'maduro'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'maduro'\n",
      "Stemmed form : maduro\n",
      "Lemmatized form : maduro\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• Maduro launches app for Venezuelans to report 'everything they see and hear'\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• Maduro launches app for Venezuelans to report 'everything they see and hear'\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• Maduro launches app for Venezuelans to report 'everything they see and hear'\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'models'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'models'\n",
      "Stemmed form : model\n",
      "Lemmatized form : model\n",
      "\n",
      "--- Matches in word_index (2 results) ---\n",
      "• The Dragon Hatchling: The missing link between the transformer and brain models\n",
      "• New coding models and integrations\n",
      "\n",
      "--- Matches in stem_index (3 results) ---\n",
      "• A Novel Spinor-Based Embedding Model for Transformers\n",
      "• The Dragon Hatchling: The missing link between the transformer and brain models\n",
      "• New coding models and integrations\n",
      "\n",
      "--- Matches in lemma_index (3 results) ---\n",
      "• A Novel Spinor-Based Embedding Model for Transformers\n",
      "• The Dragon Hatchling: The missing link between the transformer and brain models\n",
      "• New coding models and integrations\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'intel'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'intel'\n",
      "Stemmed form : intel\n",
      "Lemmatized form : intel\n",
      "\n",
      "--- Matches in word_index (2 results) ---\n",
      "• NextSilicon reveals new processor chip in challenge to Intel, AMD\n",
      "• Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM\n",
      "\n",
      "--- Matches in stem_index (2 results) ---\n",
      "• NextSilicon reveals new processor chip in challenge to Intel, AMD\n",
      "• Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM\n",
      "\n",
      "--- Matches in lemma_index (2 results) ---\n",
      "• NextSilicon reveals new processor chip in challenge to Intel, AMD\n",
      "• Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'diffs'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'diffs'\n",
      "Stemmed form : diff\n",
      "Lemmatized form : diffs\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• GitHub is working on stacked diffs\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• GitHub is working on stacked diffs\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• GitHub is working on stacked diffs\n",
      "================================================================================\n",
      "\n",
      " Keyword: 'pure'\n",
      "================================================================================\n",
      "\n",
      " Searching for: 'pure'\n",
      "Stemmed form : pure\n",
      "Lemmatized form : pure\n",
      "\n",
      "--- Matches in word_index (1 results) ---\n",
      "• We rewrote OpenFGA in pure Postgres\n",
      "\n",
      "--- Matches in stem_index (1 results) ---\n",
      "• We rewrote OpenFGA in pure Postgres\n",
      "\n",
      "--- Matches in lemma_index (1 results) ---\n",
      "• We rewrote OpenFGA in pure Postgres\n",
      "\n",
      "================================================================================\n",
      " Summary of Match Counts\n",
      "================================================================================\n",
      "Keyword        Word Index     Stem Index     Lemma Index    \n",
      "--------------------------------------------------------------------------------\n",
      "maduro         1              1              1              \n",
      "models         2              3              3              \n",
      "intel          2              2              2              \n",
      "diffs          1              1              1              \n",
      "pure           1              1              1              \n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "# Extract keywords (unique words) from article titles\n",
    "keywords = set()\n",
    "\n",
    "for title in titles:\n",
    "    words = re.findall(r\"\\b[a-zA-Z]{3,}\\b\", title.lower())  # 3+ letter words\n",
    "    keywords.update(words)\n",
    "\n",
    "# Randomly select 5 unique keywords\n",
    "sample_keywords = random.sample(list(keywords), 5)\n",
    "print(f\"\\n Randomly selected keywords: {sample_keywords}\\n\")\n",
    "\n",
    "# Search each keyword and print comparison\n",
    "comparison_results = {}\n",
    "\n",
    "for word in sample_keywords:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n Keyword: '{word}'\")\n",
    "    print(\"=\" * 80)\n",
    "    results = search_keyword(word)\n",
    "\n",
    "    # Store counts for summary\n",
    "    comparison_results[word] = {\n",
    "        \"Word Index\": len(results[\"word_index\"]),\n",
    "        \"Stem Index\": len(results[\"stem_index\"]),\n",
    "        \"Lemma Index\": len(results[\"lemma_index\"]),\n",
    "    }\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" Summary of Match Counts\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Keyword':<15}{'Word Index':<15}{'Stem Index':<15}{'Lemma Index':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for kw, counts in comparison_results.items():\n",
    "    print(f\"{kw:<15}{counts['Word Index']:<15}{counts['Stem Index']:<15}{counts['Lemma Index']:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1719268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
